{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yahooquery as yq\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = pd.read_csv('D:\\what\\ibovpred\\IBOVDia_10-07-23.csv', sep=';', encoding='latin-1')\n",
    "emp = emp['Código'] + '.SA'\n",
    "emp = emp.iloc[0:86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Código</th>\n",
       "      <th>RRRP3.SA</th>\n",
       "      <th>ALSO3.SA</th>\n",
       "      <th>ALPA4.SA</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ARZZ3.SA</th>\n",
       "      <th>ASAI3.SA</th>\n",
       "      <th>AZUL4.SA</th>\n",
       "      <th>B3SA3.SA</th>\n",
       "      <th>BBSE3.SA</th>\n",
       "      <th>BBDC3.SA</th>\n",
       "      <th>...</th>\n",
       "      <th>VIVT3.SA</th>\n",
       "      <th>TIMS3.SA</th>\n",
       "      <th>TOTS3.SA</th>\n",
       "      <th>UGPA3.SA</th>\n",
       "      <th>USIM5.SA</th>\n",
       "      <th>VALE3.SA</th>\n",
       "      <th>VIIA3.SA</th>\n",
       "      <th>VBBR3.SA</th>\n",
       "      <th>WEGE3.SA</th>\n",
       "      <th>YDUQ3.SA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.529999</td>\n",
       "      <td>28.430000</td>\n",
       "      <td>52.860001</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>96.620003</td>\n",
       "      <td>17.309999</td>\n",
       "      <td>42.549999</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>23.170000</td>\n",
       "      <td>19.272726</td>\n",
       "      <td>...</td>\n",
       "      <td>42.369999</td>\n",
       "      <td>11.54</td>\n",
       "      <td>37.740002</td>\n",
       "      <td>19.389999</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>115.070000</td>\n",
       "      <td>14.73</td>\n",
       "      <td>29.389999</td>\n",
       "      <td>35.580002</td>\n",
       "      <td>31.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.220001</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>52.630001</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>97.129997</td>\n",
       "      <td>17.610001</td>\n",
       "      <td>42.490002</td>\n",
       "      <td>16.950001</td>\n",
       "      <td>22.799999</td>\n",
       "      <td>19.336363</td>\n",
       "      <td>...</td>\n",
       "      <td>41.799999</td>\n",
       "      <td>11.77</td>\n",
       "      <td>38.090000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>115.750000</td>\n",
       "      <td>14.86</td>\n",
       "      <td>29.209999</td>\n",
       "      <td>35.180000</td>\n",
       "      <td>32.369999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.400002</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>52.490002</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>99.459999</td>\n",
       "      <td>17.542000</td>\n",
       "      <td>42.549999</td>\n",
       "      <td>17.120001</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>19.581818</td>\n",
       "      <td>...</td>\n",
       "      <td>41.720001</td>\n",
       "      <td>11.86</td>\n",
       "      <td>37.880001</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>19.790001</td>\n",
       "      <td>115.120003</td>\n",
       "      <td>14.86</td>\n",
       "      <td>29.350000</td>\n",
       "      <td>35.009998</td>\n",
       "      <td>32.759998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.549999</td>\n",
       "      <td>29.180000</td>\n",
       "      <td>52.220001</td>\n",
       "      <td>17.620001</td>\n",
       "      <td>100.809998</td>\n",
       "      <td>17.256001</td>\n",
       "      <td>41.849998</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.709999</td>\n",
       "      <td>19.490910</td>\n",
       "      <td>...</td>\n",
       "      <td>41.250000</td>\n",
       "      <td>11.87</td>\n",
       "      <td>38.480000</td>\n",
       "      <td>19.280001</td>\n",
       "      <td>19.760000</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>14.73</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>34.919998</td>\n",
       "      <td>32.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.389999</td>\n",
       "      <td>28.780001</td>\n",
       "      <td>52.180000</td>\n",
       "      <td>17.430000</td>\n",
       "      <td>99.570000</td>\n",
       "      <td>17.488001</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>19.154545</td>\n",
       "      <td>...</td>\n",
       "      <td>41.209999</td>\n",
       "      <td>11.85</td>\n",
       "      <td>37.880001</td>\n",
       "      <td>19.010000</td>\n",
       "      <td>19.330000</td>\n",
       "      <td>113.400002</td>\n",
       "      <td>14.60</td>\n",
       "      <td>28.520000</td>\n",
       "      <td>34.290001</td>\n",
       "      <td>31.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>30.340000</td>\n",
       "      <td>24.959999</td>\n",
       "      <td>9.330000</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>81.300003</td>\n",
       "      <td>13.610000</td>\n",
       "      <td>21.129999</td>\n",
       "      <td>14.740000</td>\n",
       "      <td>31.629999</td>\n",
       "      <td>14.670000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.849998</td>\n",
       "      <td>14.47</td>\n",
       "      <td>30.290001</td>\n",
       "      <td>18.719999</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>65.900002</td>\n",
       "      <td>2.17</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>37.189999</td>\n",
       "      <td>20.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>30.950001</td>\n",
       "      <td>25.370001</td>\n",
       "      <td>9.420000</td>\n",
       "      <td>15.210000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>13.720000</td>\n",
       "      <td>20.469999</td>\n",
       "      <td>14.730000</td>\n",
       "      <td>31.510000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.950001</td>\n",
       "      <td>14.37</td>\n",
       "      <td>30.830000</td>\n",
       "      <td>18.850000</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>65.320000</td>\n",
       "      <td>2.22</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>20.620001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>30.129999</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>9.120000</td>\n",
       "      <td>14.830000</td>\n",
       "      <td>83.650002</td>\n",
       "      <td>13.590000</td>\n",
       "      <td>19.139999</td>\n",
       "      <td>14.180000</td>\n",
       "      <td>31.260000</td>\n",
       "      <td>14.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.400002</td>\n",
       "      <td>14.08</td>\n",
       "      <td>29.950001</td>\n",
       "      <td>18.450001</td>\n",
       "      <td>7.370000</td>\n",
       "      <td>64.849998</td>\n",
       "      <td>2.07</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>19.950001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>32.250000</td>\n",
       "      <td>25.469999</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>83.550003</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.320000</td>\n",
       "      <td>14.360000</td>\n",
       "      <td>31.320000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.110001</td>\n",
       "      <td>14.19</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>7.530000</td>\n",
       "      <td>65.459999</td>\n",
       "      <td>2.12</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>31.580000</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>8.840000</td>\n",
       "      <td>15.060000</td>\n",
       "      <td>81.269997</td>\n",
       "      <td>13.780000</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>14.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.020000</td>\n",
       "      <td>14.15</td>\n",
       "      <td>30.230000</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>64.870003</td>\n",
       "      <td>2.05</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>36.480000</td>\n",
       "      <td>19.870001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Código   RRRP3.SA   ALSO3.SA   ALPA4.SA   ABEV3.SA    ARZZ3.SA   ASAI3.SA   \n",
       "0       46.529999  28.430000  52.860001  17.660000   96.620003  17.309999  \\\n",
       "1       46.220001  29.000000  52.630001  17.799999   97.129997  17.610001   \n",
       "2       45.400002  29.500000  52.490002  17.910000   99.459999  17.542000   \n",
       "3       43.549999  29.180000  52.220001  17.620001  100.809998  17.256001   \n",
       "4       43.389999  28.780001  52.180000  17.430000   99.570000  17.488001   \n",
       "..            ...        ...        ...        ...         ...        ...   \n",
       "494     30.340000  24.959999   9.330000  15.140000   81.300003  13.610000   \n",
       "495     30.950001  25.370001   9.420000  15.210000   85.000000  13.720000   \n",
       "496     30.129999  25.040001   9.120000  14.830000   83.650002  13.590000   \n",
       "497     32.250000  25.469999   9.200000  14.850000   83.550003  14.000000   \n",
       "498     31.580000  25.139999   8.840000  15.060000   81.269997  13.780000   \n",
       "\n",
       "Código   AZUL4.SA   B3SA3.SA   BBSE3.SA   BBDC3.SA  ...   VIVT3.SA  TIMS3.SA   \n",
       "0       42.549999  16.709999  23.170000  19.272726  ...  42.369999     11.54  \\\n",
       "1       42.490002  16.950001  22.799999  19.336363  ...  41.799999     11.77   \n",
       "2       42.549999  17.120001  23.100000  19.581818  ...  41.720001     11.86   \n",
       "3       41.849998  17.000000  22.709999  19.490910  ...  41.250000     11.87   \n",
       "4       40.500000  16.590000  22.500000  19.154545  ...  41.209999     11.85   \n",
       "..            ...        ...        ...        ...  ...        ...       ...   \n",
       "494     21.129999  14.740000  31.629999  14.670000  ...  42.849998     14.47   \n",
       "495     20.469999  14.730000  31.510000  14.690000  ...  42.950001     14.37   \n",
       "496     19.139999  14.180000  31.260000  14.350000  ...  42.400002     14.08   \n",
       "497     19.320000  14.360000  31.320000  14.500000  ...  42.110001     14.19   \n",
       "498     19.770000  14.030000  31.500000  14.550000  ...  42.020000     14.15   \n",
       "\n",
       "Código   TOTS3.SA   UGPA3.SA   USIM5.SA    VALE3.SA  VIIA3.SA   VBBR3.SA   \n",
       "0       37.740002  19.389999  20.230000  115.070000     14.73  29.389999  \\\n",
       "1       38.090000  19.400000  20.500000  115.750000     14.86  29.209999   \n",
       "2       37.880001  19.670000  19.790001  115.120003     14.86  29.350000   \n",
       "3       38.480000  19.280001  19.760000  115.480003     14.73  29.000000   \n",
       "4       37.880001  19.010000  19.330000  113.400002     14.60  28.520000   \n",
       "..            ...        ...        ...         ...       ...        ...   \n",
       "494     30.290001  18.719999   7.320000   65.900002      2.17  17.799999   \n",
       "495     30.830000  18.850000   7.380000   65.320000      2.22  18.230000   \n",
       "496     29.950001  18.450001   7.370000   64.849998      2.07  17.910000   \n",
       "497     30.500000  19.299999   7.530000   65.459999      2.12  18.309999   \n",
       "498     30.230000  19.469999   7.330000   64.870003      2.05  18.309999   \n",
       "\n",
       "Código   WEGE3.SA   YDUQ3.SA  \n",
       "0       35.580002  31.740000  \n",
       "1       35.180000  32.369999  \n",
       "2       35.009998  32.759998  \n",
       "3       34.919998  32.330002  \n",
       "4       34.290001  31.990000  \n",
       "..            ...        ...  \n",
       "494     37.189999  20.150000  \n",
       "495     37.500000  20.620001  \n",
       "496     36.889999  19.950001  \n",
       "497     37.000000  20.000000  \n",
       "498     36.480000  19.870001  \n",
       "\n",
       "[499 rows x 86 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch historical data for each stock\n",
    "stock_data = []\n",
    "stock_dataAux = []\n",
    "\n",
    "stock_dataAux = pd.DataFrame(stock_dataAux)\n",
    "\n",
    "for symbol in emp:\n",
    "    stock_data = yq.Ticker(symbol).history(period='2y', interval='1d')['close']\n",
    "    stock_data = pd.DataFrame(stock_data)\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    stock_dataAux = pd.concat([stock_dataAux, stock_data], axis=1)\n",
    "    \n",
    "stock_dataAux = stock_dataAux['close']\n",
    "stock_dataAux.columns = emp\n",
    "stock_dataAux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 86)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IBOV data\n",
    "ibov = yq.Ticker('^BVSP').history(period='2y', interval='1d')['close']\n",
    "ibov = pd.DataFrame(ibov)\n",
    "ibov.reset_index(inplace=True)\n",
    "data = pd.concat([ibov, stock_dataAux], axis=1)\n",
    "\n",
    "data = data.iloc[:, 1:87]\n",
    "#Renaming the close column to ibov\n",
    "\n",
    "data.rename(columns={'close': 'ibov'}, inplace=True)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92313221],\n",
       "       [0.96408046],\n",
       "       [1.        ],\n",
       "       [0.97701152],\n",
       "       [0.94827591],\n",
       "       [0.92887933],\n",
       "       [0.9719828 ],\n",
       "       [0.95761493],\n",
       "       [0.96839077],\n",
       "       [0.93821835],\n",
       "       [0.92959773],\n",
       "       [0.90660925],\n",
       "       [0.94252879],\n",
       "       [0.99353447],\n",
       "       [0.91091956],\n",
       "       [0.92672411],\n",
       "       [0.90660925],\n",
       "       [0.88577585],\n",
       "       [0.85057472],\n",
       "       [0.86853449],\n",
       "       [0.88936775],\n",
       "       [0.8448276 ],\n",
       "       [0.8448276 ],\n",
       "       [0.78879307],\n",
       "       [0.79310351],\n",
       "       [0.75502875],\n",
       "       [0.68390808],\n",
       "       [0.6688218 ],\n",
       "       [0.73204026],\n",
       "       [0.73419535],\n",
       "       [0.69181028],\n",
       "       [0.74353451],\n",
       "       [0.73563216],\n",
       "       [0.67744255],\n",
       "       [0.71623558],\n",
       "       [0.68031604],\n",
       "       [0.68390808],\n",
       "       [0.6688218 ],\n",
       "       [0.59913794],\n",
       "       [0.60560347],\n",
       "       [0.63433907],\n",
       "       [0.5596265 ],\n",
       "       [0.58189658],\n",
       "       [0.58117817],\n",
       "       [0.63002877],\n",
       "       [0.60488506],\n",
       "       [0.57686786],\n",
       "       [0.56537362],\n",
       "       [0.54094832],\n",
       "       [0.56034477],\n",
       "       [0.59626431],\n",
       "       [0.5912356 ],\n",
       "       [0.57399423],\n",
       "       [0.55028734],\n",
       "       [0.54741385],\n",
       "       [0.5136494 ],\n",
       "       [0.52227014],\n",
       "       [0.51293099],\n",
       "       [0.53304598],\n",
       "       [0.48419538],\n",
       "       [0.44252872],\n",
       "       [0.43965523],\n",
       "       [0.43750001],\n",
       "       [0.52658045],\n",
       "       [0.49281613],\n",
       "       [0.55387938],\n",
       "       [0.52083333],\n",
       "       [0.52873567],\n",
       "       [0.55028734],\n",
       "       [0.45977009],\n",
       "       [0.44971266],\n",
       "       [0.35704018],\n",
       "       [0.30890799],\n",
       "       [0.31681033],\n",
       "       [0.26580465],\n",
       "       [0.29956897],\n",
       "       [0.27011496],\n",
       "       [0.25862072],\n",
       "       [0.3067529 ],\n",
       "       [0.36350571],\n",
       "       [0.33692532],\n",
       "       [0.37715517],\n",
       "       [0.33764373],\n",
       "       [0.38577592],\n",
       "       [0.4123563 ],\n",
       "       [0.4899425 ],\n",
       "       [0.4856322 ],\n",
       "       [0.46767243],\n",
       "       [0.44324713],\n",
       "       [0.45977009],\n",
       "       [0.4863506 ],\n",
       "       [0.44899425],\n",
       "       [0.43965523],\n",
       "       [0.44181032],\n",
       "       [0.48132189],\n",
       "       [0.38218388],\n",
       "       [0.37715517],\n",
       "       [0.35775859],\n",
       "       [0.29022995],\n",
       "       [0.34985638],\n",
       "       [0.38218388],\n",
       "       [0.41451152],\n",
       "       [0.36063221],\n",
       "       [0.43390811],\n",
       "       [0.39870684],\n",
       "       [0.47629304],\n",
       "       [0.46839083],\n",
       "       [0.4087644 ],\n",
       "       [0.42528736],\n",
       "       [0.41379312],\n",
       "       [0.45114947],\n",
       "       [0.38218388],\n",
       "       [0.37715517],\n",
       "       [0.37428168],\n",
       "       [0.34985638],\n",
       "       [0.36135062],\n",
       "       [0.40373569],\n",
       "       [0.43821842],\n",
       "       [0.4367816 ],\n",
       "       [0.36206903],\n",
       "       [0.33764373],\n",
       "       [0.2571839 ],\n",
       "       [0.23635064],\n",
       "       [0.21982755],\n",
       "       [0.1795977 ],\n",
       "       [0.20114937],\n",
       "       [0.27155177],\n",
       "       [0.28951154],\n",
       "       [0.32183905],\n",
       "       [0.2780173 ],\n",
       "       [0.27083336],\n",
       "       [0.31393684],\n",
       "       [0.3872126 ],\n",
       "       [0.40301728],\n",
       "       [0.39583335],\n",
       "       [0.46982765],\n",
       "       [0.45330456],\n",
       "       [0.47557477],\n",
       "       [0.47126433],\n",
       "       [0.50933909],\n",
       "       [0.49137932],\n",
       "       [0.46910924],\n",
       "       [0.47916667],\n",
       "       [0.44683903],\n",
       "       [0.45043106],\n",
       "       [0.43462638],\n",
       "       [0.47198273],\n",
       "       [0.46767243],\n",
       "       [0.45043106],\n",
       "       [0.48491379],\n",
       "       [0.5143678 ],\n",
       "       [0.52227014],\n",
       "       [0.51724143],\n",
       "       [0.52227014],\n",
       "       [0.44683903],\n",
       "       [0.46982765],\n",
       "       [0.4647988 ],\n",
       "       [0.49353454],\n",
       "       [0.47772986],\n",
       "       [0.44396554],\n",
       "       [0.46048849],\n",
       "       [0.42313214],\n",
       "       [0.3347701 ],\n",
       "       [0.37571836],\n",
       "       [0.41738502],\n",
       "       [0.39798857],\n",
       "       [0.35991381],\n",
       "       [0.33548851],\n",
       "       [0.31250003],\n",
       "       [0.3096264 ],\n",
       "       [0.30747131],\n",
       "       [0.37643676],\n",
       "       [0.36422411],\n",
       "       [0.42025865],\n",
       "       [0.43247129],\n",
       "       [0.50143675],\n",
       "       [0.5352012 ],\n",
       "       [0.52370696],\n",
       "       [0.58333339],\n",
       "       [0.55459765],\n",
       "       [0.54310341],\n",
       "       [0.59841953],\n",
       "       [0.59051719],\n",
       "       [0.54885053],\n",
       "       [0.50790227],\n",
       "       [0.50000007],\n",
       "       [0.45186788],\n",
       "       [0.41954024],\n",
       "       [0.40732759],\n",
       "       [0.41882183],\n",
       "       [0.40014365],\n",
       "       [0.40373569],\n",
       "       [0.41666661],\n",
       "       [0.45833327],\n",
       "       [0.4123563 ],\n",
       "       [0.40229888],\n",
       "       [0.39008623],\n",
       "       [0.38577592],\n",
       "       [0.39870684],\n",
       "       [0.39655175],\n",
       "       [0.34913797],\n",
       "       [0.32112064],\n",
       "       [0.32543108],\n",
       "       [0.25790231],\n",
       "       [0.21910914],\n",
       "       [0.23491382],\n",
       "       [0.23635064],\n",
       "       [0.24856329],\n",
       "       [0.28663791],\n",
       "       [0.29525866],\n",
       "       [0.30818972],\n",
       "       [0.32614949],\n",
       "       [0.27083336],\n",
       "       [0.25287359],\n",
       "       [0.24640807],\n",
       "       [0.27945398],\n",
       "       [0.24640807],\n",
       "       [0.24568966],\n",
       "       [0.23491382],\n",
       "       [0.2607758 ],\n",
       "       [0.26149421],\n",
       "       [0.26795974],\n",
       "       [0.22126436],\n",
       "       [0.253592  ],\n",
       "       [0.23994254],\n",
       "       [0.21695405],\n",
       "       [0.20689649],\n",
       "       [0.22198277],\n",
       "       [0.19468398],\n",
       "       [0.16738505],\n",
       "       [0.12140808],\n",
       "       [0.10632181],\n",
       "       [0.14080467],\n",
       "       [0.11350574],\n",
       "       [0.09770119],\n",
       "       [0.08620695],\n",
       "       [0.09554597],\n",
       "       [0.08764363],\n",
       "       [0.08189651],\n",
       "       [0.08477014],\n",
       "       [0.06106325],\n",
       "       [0.04022985],\n",
       "       [0.03089083],\n",
       "       [0.05747121],\n",
       "       [0.06250006],\n",
       "       [0.06537355],\n",
       "       [0.06034484],\n",
       "       [0.07543098],\n",
       "       [0.08836204],\n",
       "       [0.04525856],\n",
       "       [0.06250006],\n",
       "       [0.06034484],\n",
       "       [0.03304605],\n",
       "       [0.03376432],\n",
       "       [0.03520113],\n",
       "       [0.04022985],\n",
       "       [0.05747121],\n",
       "       [0.07183908],\n",
       "       [0.07902302],\n",
       "       [0.07614939],\n",
       "       [0.05675294],\n",
       "       [0.09267248],\n",
       "       [0.10704021],\n",
       "       [0.09267248],\n",
       "       [0.11206893],\n",
       "       [0.0984196 ],\n",
       "       [0.1264368 ],\n",
       "       [0.2083333 ],\n",
       "       [0.22988511],\n",
       "       [0.22988511],\n",
       "       [0.18678164],\n",
       "       [0.23706891],\n",
       "       [0.2076149 ],\n",
       "       [0.24784488],\n",
       "       [0.24568966],\n",
       "       [0.24137935],\n",
       "       [0.23706891],\n",
       "       [0.22557467],\n",
       "       [0.21048853],\n",
       "       [0.19181035],\n",
       "       [0.20114937],\n",
       "       [0.21408043],\n",
       "       [0.21048853],\n",
       "       [0.17385058],\n",
       "       [0.16882187],\n",
       "       [0.14295975],\n",
       "       [0.14367816],\n",
       "       [0.14655179],\n",
       "       [0.17385058],\n",
       "       [0.19899428],\n",
       "       [0.16307474],\n",
       "       [0.18462641],\n",
       "       [0.23419542],\n",
       "       [0.23491382],\n",
       "       [0.20258618],\n",
       "       [0.21336202],\n",
       "       [0.20258618],\n",
       "       [0.20114937],\n",
       "       [0.22126436],\n",
       "       [0.20905171],\n",
       "       [0.23635064],\n",
       "       [0.27729889],\n",
       "       [0.30172419],\n",
       "       [0.27442526],\n",
       "       [0.2852011 ],\n",
       "       [0.3096264 ],\n",
       "       [0.27658049],\n",
       "       [0.3103448 ],\n",
       "       [0.39080463],\n",
       "       [0.387931  ],\n",
       "       [0.39942525],\n",
       "       [0.39655175],\n",
       "       [0.38936782],\n",
       "       [0.38936782],\n",
       "       [0.39152304],\n",
       "       [0.37931039],\n",
       "       [0.35632191],\n",
       "       [0.36997123],\n",
       "       [0.37571836],\n",
       "       [0.37859198],\n",
       "       [0.37212646],\n",
       "       [0.40660918],\n",
       "       [0.3800288 ],\n",
       "       [0.37428168],\n",
       "       [0.33261488],\n",
       "       [0.38074707],\n",
       "       [0.37428168],\n",
       "       [0.44181032],\n",
       "       [0.44899425],\n",
       "       [0.47916667],\n",
       "       [0.4892241 ],\n",
       "       [0.41091949],\n",
       "       [0.41522993],\n",
       "       [0.39583335],\n",
       "       [0.25574709],\n",
       "       [0.18750004],\n",
       "       [0.19612066],\n",
       "       [0.11997127],\n",
       "       [0.10344831],\n",
       "       [0.1307471 ],\n",
       "       [0.19755747],\n",
       "       [0.17744248],\n",
       "       [0.16379315],\n",
       "       [0.21551724],\n",
       "       [0.16091952],\n",
       "       [0.14942528],\n",
       "       [0.18893672],\n",
       "       [0.16738505],\n",
       "       [0.13146551],\n",
       "       [0.15804603],\n",
       "       [0.11206893],\n",
       "       [0.13146551],\n",
       "       [0.1300287 ],\n",
       "       [0.08405173],\n",
       "       [0.07614939],\n",
       "       [0.04669537],\n",
       "       [0.0143678 ],\n",
       "       [0.03089083],\n",
       "       [0.03304605],\n",
       "       [0.02370689],\n",
       "       [0.03376432],\n",
       "       [0.07830461],\n",
       "       [0.07543098],\n",
       "       [0.07327589],\n",
       "       [0.13649422],\n",
       "       [0.10919543],\n",
       "       [0.06609196],\n",
       "       [0.11637937],\n",
       "       [0.10272991],\n",
       "       [0.05028741],\n",
       "       [0.        ],\n",
       "       [0.01077583],\n",
       "       [0.04741378],\n",
       "       [0.0811781 ],\n",
       "       [0.11063225],\n",
       "       [0.14942528],\n",
       "       [0.19468398],\n",
       "       [0.17385058],\n",
       "       [0.14942528],\n",
       "       [0.13577582],\n",
       "       [0.1515805 ],\n",
       "       [0.15589081],\n",
       "       [0.15373559],\n",
       "       [0.1307471 ],\n",
       "       [0.1307471 ],\n",
       "       [0.15229891],\n",
       "       [0.17313217],\n",
       "       [0.18175292],\n",
       "       [0.15732762],\n",
       "       [0.16163793],\n",
       "       [0.17456899],\n",
       "       [0.16666664],\n",
       "       [0.18678164],\n",
       "       [0.13290233],\n",
       "       [0.16451156],\n",
       "       [0.14655179],\n",
       "       [0.16954027],\n",
       "       [0.13218392],\n",
       "       [0.13577582],\n",
       "       [0.14152294],\n",
       "       [0.12356317],\n",
       "       [0.13936785],\n",
       "       [0.15804603],\n",
       "       [0.17241376],\n",
       "       [0.15445399],\n",
       "       [0.15445399],\n",
       "       [0.13864945],\n",
       "       [0.14224135],\n",
       "       [0.10775862],\n",
       "       [0.05531612],\n",
       "       [0.05962643],\n",
       "       [0.06609196],\n",
       "       [0.09339075],\n",
       "       [0.09339075],\n",
       "       [0.14080467],\n",
       "       [0.14080467],\n",
       "       [0.1271552 ],\n",
       "       [0.1271552 ],\n",
       "       [0.13146551],\n",
       "       [0.15229891],\n",
       "       [0.14583338],\n",
       "       [0.14583338],\n",
       "       [0.1020115 ],\n",
       "       [0.09770119],\n",
       "       [0.09339075],\n",
       "       [0.02298848],\n",
       "       [0.11135052],\n",
       "       [0.13649422],\n",
       "       [0.14295975],\n",
       "       [0.14583338],\n",
       "       [0.17744248],\n",
       "       [0.14511497],\n",
       "       [0.11853445],\n",
       "       [0.14224135],\n",
       "       [0.15660922],\n",
       "       [0.16235634],\n",
       "       [0.16235634],\n",
       "       [0.19468398],\n",
       "       [0.19899428],\n",
       "       [0.21910914],\n",
       "       [0.21839087],\n",
       "       [0.2083333 ],\n",
       "       [0.19827588],\n",
       "       [0.18103451],\n",
       "       [0.20258618],\n",
       "       [0.18893672],\n",
       "       [0.17313217],\n",
       "       [0.1515805 ],\n",
       "       [0.18534482],\n",
       "       [0.21982755],\n",
       "       [0.18103451],\n",
       "       [0.204023  ],\n",
       "       [0.23922413],\n",
       "       [0.26364943],\n",
       "       [0.23635064],\n",
       "       [0.24497125],\n",
       "       [0.27370686],\n",
       "       [0.27442526],\n",
       "       [0.27442526],\n",
       "       [0.29094822],\n",
       "       [0.29310344],\n",
       "       [0.32758617],\n",
       "       [0.34913797],\n",
       "       [0.36925283],\n",
       "       [0.3836207 ],\n",
       "       [0.38146548],\n",
       "       [0.37643676],\n",
       "       [0.39942525],\n",
       "       [0.4123563 ],\n",
       "       [0.42887926],\n",
       "       [0.4367816 ],\n",
       "       [0.43750001],\n",
       "       [0.49568963],\n",
       "       [0.48275857],\n",
       "       [0.47557477],\n",
       "       [0.53376439],\n",
       "       [0.51867811],\n",
       "       [0.51293099],\n",
       "       [0.53951151],\n",
       "       [0.51293099],\n",
       "       [0.54597704],\n",
       "       [0.55100575],\n",
       "       [0.56178158],\n",
       "       [0.57040233],\n",
       "       [0.60129316],\n",
       "       [0.65517247],\n",
       "       [0.63362066],\n",
       "       [0.6939655 ],\n",
       "       [0.67097702],\n",
       "       [0.63936779],\n",
       "       [0.61925293],\n",
       "       [0.62787354],\n",
       "       [0.65158043],\n",
       "       [0.68462648],\n",
       "       [0.67385051],\n",
       "       [0.70330466],\n",
       "       [0.67959777],\n",
       "       [0.71048846],\n",
       "       [0.68678157]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data_treinamento = data.iloc[:, 2:86].values\n",
    "\n",
    "normalizador = MinMaxScaler(feature_range=(0,1))\n",
    "data_treinamento_normalizada = normalizador.fit_transform(data_treinamento)\n",
    "\n",
    "normalizador_previsao = MinMaxScaler(feature_range=(0,1))\n",
    "normalizador_previsao.fit_transform(data_treinamento[:,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = []\n",
    "preco_real = []\n",
    "\n",
    "for i in range(15, 316):\n",
    "    previsores.append(data_treinamento_normalizada[i-15:i, :])\n",
    "    preco_real.append(data_treinamento_normalizada[i, 0])\n",
    "previsores, preco_real = np.array(previsores), np.array(preco_real)\n",
    "\n",
    "# Validate and reshape the input shape of previsores\n",
    "num_samples, timesteps, num_features = previsores.shape\n",
    "previsores_reshaped = previsores.reshape(num_samples, timesteps, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0379 - mean_absolute_error: 0.1536\n",
      "Epoch 1: loss improved from inf to 0.03706, saving model to pesos.h5\n",
      "10/10 [==============================] - 7s 30ms/step - loss: 0.0371 - mean_absolute_error: 0.1528 - lr: 0.0010\n",
      "Epoch 2/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0226 - mean_absolute_error: 0.1205\n",
      "Epoch 2: loss improved from 0.03706 to 0.02346, saving model to pesos.h5\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.0235 - mean_absolute_error: 0.1218 - lr: 0.0010\n",
      "Epoch 3/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0182 - mean_absolute_error: 0.1061\n",
      "Epoch 3: loss improved from 0.02346 to 0.01816, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 0.0182 - mean_absolute_error: 0.1056 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0134 - mean_absolute_error: 0.0900\n",
      "Epoch 4: loss improved from 0.01816 to 0.01340, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0134 - mean_absolute_error: 0.0900 - lr: 0.0010\n",
      "Epoch 5/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0136 - mean_absolute_error: 0.0911\n",
      "Epoch 5: loss improved from 0.01340 to 0.01320, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0132 - mean_absolute_error: 0.0901 - lr: 0.0010\n",
      "Epoch 6/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0095 - mean_absolute_error: 0.0746\n",
      "Epoch 6: loss improved from 0.01320 to 0.01064, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0106 - mean_absolute_error: 0.0791 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0129 - mean_absolute_error: 0.0918\n",
      "Epoch 7: loss did not improve from 0.01064\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0129 - mean_absolute_error: 0.0918 - lr: 0.0010\n",
      "Epoch 8/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0098 - mean_absolute_error: 0.0794\n",
      "Epoch 8: loss improved from 0.01064 to 0.00997, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0100 - mean_absolute_error: 0.0793 - lr: 0.0010\n",
      "Epoch 9/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0087 - mean_absolute_error: 0.0746\n",
      "Epoch 9: loss improved from 0.00997 to 0.00877, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 51ms/step - loss: 0.0088 - mean_absolute_error: 0.0747 - lr: 0.0010\n",
      "Epoch 10/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0078 - mean_absolute_error: 0.0676\n",
      "Epoch 10: loss improved from 0.00877 to 0.00806, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0081 - mean_absolute_error: 0.0690 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0722\n",
      "Epoch 11: loss did not improve from 0.00806\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0082 - mean_absolute_error: 0.0722 - lr: 0.0010\n",
      "Epoch 12/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0073 - mean_absolute_error: 0.0672\n",
      "Epoch 12: loss improved from 0.00806 to 0.00708, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.0071 - mean_absolute_error: 0.0661 - lr: 0.0010\n",
      "Epoch 13/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0071 - mean_absolute_error: 0.0660\n",
      "Epoch 13: loss improved from 0.00708 to 0.00688, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0069 - mean_absolute_error: 0.0650 - lr: 0.0010\n",
      "Epoch 14/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0070 - mean_absolute_error: 0.0660\n",
      "Epoch 14: loss did not improve from 0.00688\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0071 - mean_absolute_error: 0.0662 - lr: 0.0010\n",
      "Epoch 15/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0072 - mean_absolute_error: 0.0666\n",
      "Epoch 15: loss did not improve from 0.00688\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0071 - mean_absolute_error: 0.0658 - lr: 0.0010\n",
      "Epoch 16/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0060 - mean_absolute_error: 0.0607\n",
      "Epoch 16: loss improved from 0.00688 to 0.00625, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0062 - mean_absolute_error: 0.0620 - lr: 0.0010\n",
      "Epoch 17/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0063 - mean_absolute_error: 0.0630\n",
      "Epoch 17: loss improved from 0.00625 to 0.00610, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.0061 - mean_absolute_error: 0.0614 - lr: 0.0010\n",
      "Epoch 18/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0067 - mean_absolute_error: 0.0653\n",
      "Epoch 18: loss did not improve from 0.00610\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0068 - mean_absolute_error: 0.0662 - lr: 0.0010\n",
      "Epoch 19/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0068 - mean_absolute_error: 0.0658\n",
      "Epoch 19: loss did not improve from 0.00610\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0065 - mean_absolute_error: 0.0639 - lr: 0.0010\n",
      "Epoch 20/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0048 - mean_absolute_error: 0.0554\n",
      "Epoch 20: loss improved from 0.00610 to 0.00494, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0049 - mean_absolute_error: 0.0563 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0053 - mean_absolute_error: 0.0556\n",
      "Epoch 21: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0053 - mean_absolute_error: 0.0556 - lr: 0.0010\n",
      "Epoch 22/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - mean_absolute_error: 0.0568\n",
      "Epoch 22: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0054 - mean_absolute_error: 0.0565 - lr: 0.0010\n",
      "Epoch 23/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0061 - mean_absolute_error: 0.0615\n",
      "Epoch 23: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 0.0061 - mean_absolute_error: 0.0614 - lr: 0.0010\n",
      "Epoch 24/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0063 - mean_absolute_error: 0.0632\n",
      "Epoch 24: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0063 - mean_absolute_error: 0.0627 - lr: 0.0010\n",
      "Epoch 25/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - mean_absolute_error: 0.0592\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 25: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0055 - mean_absolute_error: 0.0594 - lr: 0.0010\n",
      "Epoch 26/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0054 - mean_absolute_error: 0.0585\n",
      "Epoch 26: loss did not improve from 0.00494\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0056 - mean_absolute_error: 0.0593 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0047 - mean_absolute_error: 0.0533\n",
      "Epoch 27: loss improved from 0.00494 to 0.00452, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 52ms/step - loss: 0.0045 - mean_absolute_error: 0.0522 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0045 - mean_absolute_error: 0.0519\n",
      "Epoch 28: loss did not improve from 0.00452\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0046 - mean_absolute_error: 0.0530 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0515\n",
      "Epoch 29: loss improved from 0.00452 to 0.00420, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 0.0042 - mean_absolute_error: 0.0511 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0492\n",
      "Epoch 30: loss improved from 0.00420 to 0.00380, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 0.0038 - mean_absolute_error: 0.0492 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0488\n",
      "Epoch 31: loss did not improve from 0.00380\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0039 - mean_absolute_error: 0.0477 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0500\n",
      "Epoch 32: loss did not improve from 0.00380\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0040 - mean_absolute_error: 0.0497 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0486\n",
      "Epoch 33: loss improved from 0.00380 to 0.00371, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.0037 - mean_absolute_error: 0.0481 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0487\n",
      "Epoch 34: loss did not improve from 0.00371\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0038 - mean_absolute_error: 0.0484 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0470\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 35: loss did not improve from 0.00371\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0038 - mean_absolute_error: 0.0464 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0484\n",
      "Epoch 36: loss improved from 0.00371 to 0.00363, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 0.0036 - mean_absolute_error: 0.0481 - lr: 4.0000e-05\n",
      "Epoch 37/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0481\n",
      "Epoch 37: loss did not improve from 0.00363\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0038 - mean_absolute_error: 0.0480 - lr: 4.0000e-05\n",
      "Epoch 38/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0465\n",
      "Epoch 38: loss improved from 0.00363 to 0.00361, saving model to pesos.h5\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.0036 - mean_absolute_error: 0.0466 - lr: 4.0000e-05\n",
      "Epoch 39/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0479\n",
      "Epoch 39: loss did not improve from 0.00361\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0037 - mean_absolute_error: 0.0476 - lr: 4.0000e-05\n",
      "Epoch 40/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0471\n",
      "Epoch 40: loss did not improve from 0.00361\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0037 - mean_absolute_error: 0.0462 - lr: 4.0000e-05\n",
      "Epoch 41/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0465\n",
      "Epoch 41: loss improved from 0.00361 to 0.00348, saving model to pesos.h5\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.0035 - mean_absolute_error: 0.0464 - lr: 4.0000e-05\n",
      "Epoch 42/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0454\n",
      "Epoch 42: loss improved from 0.00348 to 0.00343, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 0.0034 - mean_absolute_error: 0.0450 - lr: 4.0000e-05\n",
      "Epoch 43/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0463\n",
      "Epoch 43: loss did not improve from 0.00343\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0036 - mean_absolute_error: 0.0464 - lr: 4.0000e-05\n",
      "Epoch 44/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0466\n",
      "Epoch 44: loss did not improve from 0.00343\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0036 - mean_absolute_error: 0.0467 - lr: 4.0000e-05\n",
      "Epoch 45/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0497\n",
      "Epoch 45: loss did not improve from 0.00343\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0039 - mean_absolute_error: 0.0491 - lr: 4.0000e-05\n",
      "Epoch 46/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0446\n",
      "Epoch 46: loss improved from 0.00343 to 0.00332, saving model to pesos.h5\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.0033 - mean_absolute_error: 0.0442 - lr: 4.0000e-05\n",
      "Epoch 47/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0473\n",
      "Epoch 47: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 0.0037 - mean_absolute_error: 0.0472 - lr: 4.0000e-05\n",
      "Epoch 48/100\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0449\n",
      "Epoch 48: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0035 - mean_absolute_error: 0.0462 - lr: 4.0000e-05\n",
      "Epoch 49/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0458\n",
      "Epoch 49: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0035 - mean_absolute_error: 0.0452 - lr: 4.0000e-05\n",
      "Epoch 50/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0486\n",
      "Epoch 50: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0036 - mean_absolute_error: 0.0474 - lr: 4.0000e-05\n",
      "Epoch 51/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0461\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 51: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0036 - mean_absolute_error: 0.0459 - lr: 4.0000e-05\n",
      "Epoch 52/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0459\n",
      "Epoch 52: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 0.0036 - mean_absolute_error: 0.0461 - lr: 8.0000e-06\n",
      "Epoch 53/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0454\n",
      "Epoch 53: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0034 - mean_absolute_error: 0.0449 - lr: 8.0000e-06\n",
      "Epoch 54/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0476\n",
      "Epoch 54: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0036 - mean_absolute_error: 0.0471 - lr: 8.0000e-06\n",
      "Epoch 55/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0470\n",
      "Epoch 55: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0037 - mean_absolute_error: 0.0472 - lr: 8.0000e-06\n",
      "Epoch 56/100\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0459\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 56: loss did not improve from 0.00332\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.0035 - mean_absolute_error: 0.0463 - lr: 8.0000e-06\n",
      "Epoch 56: early stopping\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 100, return_sequences = True, input_shape = (previsores.shape[1], 84)))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "regressor.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "es = EarlyStopping(monitor='loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='pesos.h5', monitor='loss', save_best_only=True, verbose=1)\n",
    "\n",
    "regressor.fit(previsores, preco_real, epochs = 100, batch_size = 32, callbacks=[es, rlr, mcp])\n",
    "\n",
    "regressor.save_weights('regressor_ibov.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
